{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db1d752b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Asus\\anaconda3\\envs\\kidney\\lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Ensure eager execution is enabled\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "# Optional: reset any previous session\n",
    "tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c91dd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dfd3f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"C:\\Users\\Asus\\Desktop\\kidney_Disease_Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "924ede46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Asus\\\\Desktop\\\\kidney_Disease_Classifier'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb75fafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "import yaml\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2be77a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_entity.py\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    root_dir: Path\n",
    "    trained_model_path: Path\n",
    "    updated_base_model_path: Path\n",
    "    training_data: Path\n",
    "    \n",
    "    # Core training params\n",
    "    params_epochs: int\n",
    "    params_batch_size: int\n",
    "    params_is_augmentation: bool\n",
    "    params_image_size: List[int]       # matches IMAGE_SIZE in params.yaml\n",
    "    params_learning_rate: float        # matches LEARNING_RATE\n",
    "    params_classes: int                # matches CLASSES\n",
    "    params_freeze_till: int            # matches FREEZE_TILL\n",
    "    params_fine_tune_last_n: int       # matches FINE_TUNE_LAST_N\n",
    "    \n",
    "    # Regularization (anti-overfitting)\n",
    "    params_dropout_rate_head: float    # matches DROPOUT_RATE_HEAD\n",
    "    params_weight_decay: float         # matches WEIGHT_DECAY\n",
    "    params_label_smoothing: float      # matches LABEL_SMOOTHING\n",
    "    \n",
    "    # Model architecture\n",
    "    params_dense_units: int            # matches DENSE_UNITS\n",
    "    params_optimizer: str              # matches OPTIMIZER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57b55d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnnClassifier.constants import *\n",
    "from cnnClassifier.utils.common  import read_yaml, create_directories\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "272c12e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnnClassifier.entity.config_entity import (\n",
    "    DataIngestionConfig,\n",
    "    PrepareBaseModelConfig,\n",
    "    TrainingConfig\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caac0ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self, config_filepath=CONFIG_FILE_PATH, params_filepath=PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        create_directories([self.config['artifacts_root']])\n",
    "    \n",
    "    def get_training_config(self) -> TrainingConfig:\n",
    "        training = self.config['training']\n",
    "        prepare_base_model = self.config['prepare_base_model']\n",
    "        params = self.params\n",
    "\n",
    "        training_data = os.path.join(\n",
    "            self.config['data_ingestion']['unzip_dir'],\n",
    "            \"CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone\",\n",
    "            \"CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone\",\n",
    "        )\n",
    "\n",
    "        create_directories([Path(training['root_dir'])])\n",
    "\n",
    "        \n",
    "        return TrainingConfig(\n",
    "        root_dir=Path(training['root_dir']),\n",
    "        trained_model_path=Path(training['trained_model_path']),\n",
    "        updated_base_model_path=Path(prepare_base_model['updated_base_model_path']),\n",
    "        training_data=Path(training_data),\n",
    "\n",
    "        # --- Core training params ---\n",
    "        params_epochs=params.get('EPOCHS', 25),\n",
    "        params_batch_size=params.get('BATCH_SIZE', 16),\n",
    "        params_is_augmentation=params.get('AUGMENTATION', True),\n",
    "        params_image_size=params.get('IMAGE_SIZE', [224, 224, 3]),\n",
    "        params_learning_rate=params.get('LEARNING_RATE', 1e-4),\n",
    "        params_classes=params.get('CLASSES', 4),\n",
    "        params_freeze_till=params.get('FREEZE_TILL', 120),\n",
    "        params_fine_tune_last_n=params.get('FINE_TUNE_LAST_N', 60),\n",
    "\n",
    "        # --- Regularization ---\n",
    "        params_dropout_rate_head=params.get('DROPOUT_RATE_HEAD', 0.4),\n",
    "        params_weight_decay=params.get('WEIGHT_DECAY', 0.0002),\n",
    "        params_label_smoothing=params.get('LABEL_SMOOTHING', 0.1),\n",
    "\n",
    "        # --- Model architecture tweaks ---\n",
    "        params_dense_units=params.get('DENSE_UNITS', 512),\n",
    "        params_optimizer=params.get('OPTIMIZER', \"adamw\"),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56090eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request as request\n",
    "from zipfile import ZipFile\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c550b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from cnnClassifier.entity.config_entity import TrainingConfig\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "\n",
    "\n",
    "class Training:\n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "        self.model: tf.keras.Model | None = None\n",
    "        self.train_generator: tf.keras.preprocessing.image.DirectoryIterator | None = None\n",
    "        self.valid_generator: tf.keras.preprocessing.image.DirectoryIterator | None = None\n",
    "\n",
    "    # -----------------------\n",
    "    # Load base model\n",
    "    # -----------------------\n",
    "    def get_base_model(self) -> tf.keras.Model:\n",
    "        path = str(self.config.updated_base_model_path)\n",
    "        if not path.endswith(\".keras\") and not path.endswith(\".h5\"):\n",
    "            path += \".keras\"\n",
    "        self.model = tf.keras.models.load_model(path)\n",
    "        print(f\"âœ… Model loaded from: {path}\")\n",
    "        return self.model\n",
    "\n",
    "    # -----------------------\n",
    "    # Unfreeze last N layers\n",
    "    # -----------------------\n",
    "    def unfreeze_top_layers(self, num_layers: int = None, learning_rate: float = None):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Load the model first using get_base_model()\")\n",
    "\n",
    "        if num_layers is None:\n",
    "            num_layers = self.config.params_fine_tune_last_n\n",
    "        if learning_rate is None:\n",
    "            learning_rate = self.config.params_learning_rate\n",
    "\n",
    "        # Unfreeze selected layers\n",
    "        for layer in self.model.layers[-num_layers:]:\n",
    "            layer.trainable = True\n",
    "\n",
    "        print(f\"âœ… Top {num_layers} layers set to trainable for fine-tuning.\")\n",
    "\n",
    "        # Recompile model\n",
    "        self.model.compile(\n",
    "            optimizer=AdamW(\n",
    "                learning_rate=learning_rate,\n",
    "                weight_decay=self.config.params_weight_decay\n",
    "            ),\n",
    "            loss=tf.keras.losses.CategoricalCrossentropy(\n",
    "                label_smoothing=self.config.params_label_smoothing\n",
    "            ),\n",
    "            metrics=[\n",
    "                \"accuracy\",\n",
    "                tf.keras.metrics.AUC(name=\"auc\"),\n",
    "                tf.keras.metrics.Precision(name=\"precision\"),\n",
    "                tf.keras.metrics.Recall(name=\"recall\")\n",
    "            ]\n",
    "        )\n",
    "        print(f\"âœ… Model recompiled after unfreezing layers (lr={learning_rate}).\")\n",
    "\n",
    "    # -----------------------\n",
    "    # Generators\n",
    "    # -----------------------\n",
    "    def train_valid_generator(self):\n",
    "        datagenerator_kwargs = dict(\n",
    "            rescale=1.0 / 255,\n",
    "            validation_split=0.2,\n",
    "        )\n",
    "        dataflow_kwargs = dict(\n",
    "            target_size=self.config.params_image_size[:2],\n",
    "            batch_size=self.config.params_batch_size,\n",
    "            interpolation=\"bilinear\",\n",
    "            class_mode=\"categorical\"\n",
    "        )\n",
    "\n",
    "        # Validation generator\n",
    "        valid_datagenerator = tf.keras.preprocessing.image.ImageDataGenerator(**datagenerator_kwargs)\n",
    "        self.valid_generator = valid_datagenerator.flow_from_directory(\n",
    "            str(self.config.training_data),\n",
    "            subset=\"validation\",\n",
    "            shuffle=False,\n",
    "            seed=42,\n",
    "            **dataflow_kwargs\n",
    "        )\n",
    "\n",
    "        # Training generator with augmentation\n",
    "        if self.config.params_is_augmentation:\n",
    "            train_datagenerator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "                rescale=1.0 / 255,\n",
    "                rotation_range=20,\n",
    "                zoom_range=0.2,\n",
    "                horizontal_flip=True,\n",
    "                vertical_flip=False,\n",
    "                brightness_range=[0.8, 1.2],\n",
    "                fill_mode=\"nearest\",\n",
    "                validation_split=0.2\n",
    "            )\n",
    "        else:\n",
    "            train_datagenerator = valid_datagenerator\n",
    "\n",
    "        self.train_generator = train_datagenerator.flow_from_directory(\n",
    "            str(self.config.training_data),\n",
    "            subset=\"training\",\n",
    "            shuffle=True,\n",
    "            seed=42,\n",
    "            **dataflow_kwargs\n",
    "        )\n",
    "\n",
    "    # -----------------------\n",
    "    # Callbacks\n",
    "    # -----------------------\n",
    "    def get_callbacks(self):\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_accuracy\",\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "            mode=\"max\",\n",
    "            verbose=1\n",
    "        )\n",
    "        reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor=\"val_accuracy\",\n",
    "            factor=0.2,\n",
    "            patience=3,\n",
    "            verbose=1,\n",
    "            min_lr=1e-7\n",
    "        )\n",
    "        checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=str(self.config.trained_model_path),\n",
    "            save_best_only=True,\n",
    "            monitor=\"val_accuracy\",\n",
    "            mode=\"max\",\n",
    "            verbose=1\n",
    "        )\n",
    "        return [early_stopping, reduce_lr, checkpoint]\n",
    "\n",
    "    # -----------------------\n",
    "    # Training with progressive unfreezing\n",
    "    # -----------------------\n",
    "    def train(self):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not loaded. Call get_base_model() first.\")\n",
    "        if self.train_generator is None or self.valid_generator is None:\n",
    "            raise ValueError(\"Generators not prepared. Call train_valid_generator() first.\")\n",
    "\n",
    "        callbacks = self.get_callbacks()\n",
    "\n",
    "        # Progressive unfreezing schedule (layers, learning_rate)\n",
    "        unfreeze_schedule = [\n",
    "            (10, 1e-4),   # Phase 1\n",
    "            (20, 5e-5),   # Phase 2\n",
    "            \n",
    "        ]\n",
    "\n",
    "        history = None\n",
    "        for i, (layers, lr) in enumerate(unfreeze_schedule, 1):\n",
    "            print(f\"\\nğŸš€ Phase {i}: Unfreezing last {layers} layers with lr={lr}...\")\n",
    "            self.unfreeze_top_layers(num_layers=layers, learning_rate=lr)\n",
    "            history = self.model.fit(\n",
    "                self.train_generator,\n",
    "                validation_data=self.valid_generator,\n",
    "                epochs=5,  # per phase\n",
    "                callbacks=callbacks\n",
    "            )\n",
    "\n",
    "        # Save final model (best checkpoint already saved)\n",
    "        self.save_model(self.config.trained_model_path, self.model)\n",
    "        return history\n",
    "\n",
    "    # -----------------------\n",
    "    # Save model\n",
    "    # -----------------------\n",
    "    @staticmethod\n",
    "    def save_model(path: Path, model: tf.keras.Model):\n",
    "        path_str = str(path)\n",
    "        if not path_str.endswith(\".keras\") and not path_str.endswith(\".h5\"):\n",
    "            path_str += \".keras\"\n",
    "        model.save(path_str, include_optimizer=False)\n",
    "        print(f\"âœ… Model saved at: {path_str}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "494447bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-20 14:48:42,213: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-09-20 14:48:42,216: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-09-20 14:48:42,219: INFO: common: created directory at: artifacts_root]\n",
      "[2025-09-20 14:48:42,220: INFO: common: created directory at: artifacts\\training]\n",
      "âœ… Model loaded from: artifacts\\prepare_base_model\\updated_base_model.keras\n",
      "âœ… Top 40 layers set to trainable for fine-tuning.\n",
      "âœ… Model recompiled after unfreezing layers (lr=5e-05).\n",
      "Found 2487 images belonging to 4 classes.\n",
      "Found 9959 images belonging to 4 classes.\n",
      "\n",
      "ğŸš€ Phase 1: Unfreezing last 10 layers with lr=0.0001...\n",
      "âœ… Top 10 layers set to trainable for fine-tuning.\n",
      "âœ… Model recompiled after unfreezing layers (lr=0.0001).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\anaconda3\\envs\\kidney\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m623/623\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7429 - auc: 0.9127 - loss: 0.9892 - precision: 0.8595 - recall: 0.5942\n",
      "Epoch 1: val_accuracy improved from None to 0.80901, saving model to artifacts\\training\\model.keras\n",
      "\u001b[1m623/623\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m968s\u001b[0m 2s/step - accuracy: 0.8738 - auc: 0.9805 - loss: 0.7765 - precision: 0.9390 - recall: 0.8039 - val_accuracy: 0.8090 - val_auc: 0.9453 - val_loss: 0.9243 - val_precision: 0.8274 - val_recall: 0.7901 - learning_rate: 1.0000e-04\n",
      "Epoch 2/5\n",
      "\u001b[1m623/623\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9818 - auc: 0.9991 - loss: 0.5767 - precision: 0.9884 - recall: 0.9711\n",
      "Epoch 2: val_accuracy did not improve from 0.80901\n",
      "\u001b[1m623/623\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m978s\u001b[0m 2s/step - accuracy: 0.9840 - auc: 0.9993 - loss: 0.5635 - precision: 0.9894 - recall: 0.9763 - val_accuracy: 0.7415 - val_auc: 0.9025 - val_loss: 1.1070 - val_precision: 0.7507 - val_recall: 0.7266 - learning_rate: 1.0000e-04\n",
      "Epoch 3/5\n",
      "\u001b[1m623/623\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9904 - auc: 0.9997 - loss: 0.5347 - precision: 0.9935 - recall: 0.9843\n",
      "Epoch 3: val_accuracy improved from 0.80901 to 0.81745, saving model to artifacts\\training\\model.keras\n",
      "\u001b[1m623/623\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m978s\u001b[0m 2s/step - accuracy: 0.9915 - auc: 0.9997 - loss: 0.5261 - precision: 0.9939 - recall: 0.9872 - val_accuracy: 0.8175 - val_auc: 0.9476 - val_loss: 0.8585 - val_precision: 0.8325 - val_recall: 0.7913 - learning_rate: 1.0000e-04\n",
      "Epoch 4/5\n",
      "\u001b[1m623/623\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9955 - auc: 0.9998 - loss: 0.5039 - precision: 0.9964 - recall: 0.9927\n",
      "Epoch 4: val_accuracy did not improve from 0.81745\n",
      "\u001b[1m623/623\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m981s\u001b[0m 2s/step - accuracy: 0.9924 - auc: 0.9997 - loss: 0.5040 - precision: 0.9937 - recall: 0.9901 - val_accuracy: 0.8170 - val_auc: 0.9422 - val_loss: 0.8576 - val_precision: 0.8289 - val_recall: 0.7986 - learning_rate: 1.0000e-04\n",
      "Epoch 5/5\n",
      "\u001b[1m623/623\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9961 - auc: 0.9999 - loss: 0.4813 - precision: 0.9971 - recall: 0.9950\n",
      "Epoch 5: val_accuracy did not improve from 0.81745\n",
      "\u001b[1m623/623\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m986s\u001b[0m 2s/step - accuracy: 0.9964 - auc: 0.9998 - loss: 0.4772 - precision: 0.9971 - recall: 0.9948 - val_accuracy: 0.8106 - val_auc: 0.9405 - val_loss: 0.8755 - val_precision: 0.8172 - val_recall: 0.7965 - learning_rate: 1.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\n",
      "ğŸš€ Phase 2: Unfreezing last 20 layers with lr=5e-05...\n",
      "âœ… Top 20 layers set to trainable for fine-tuning.\n",
      "âœ… Model recompiled after unfreezing layers (lr=5e-05).\n",
      "Epoch 1/5\n",
      "\u001b[1m623/623\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9941 - auc: 0.9998 - loss: 0.5013 - precision: 0.9952 - recall: 0.9922\n",
      "Epoch 1: val_accuracy improved from 0.81745 to 0.82509, saving model to artifacts\\training\\model.keras\n",
      "\u001b[1m623/623\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1012s\u001b[0m 2s/step - accuracy: 0.9953 - auc: 0.9999 - loss: 0.4907 - precision: 0.9967 - recall: 0.9936 - val_accuracy: 0.8251 - val_auc: 0.9526 - val_loss: 0.8197 - val_precision: 0.8407 - val_recall: 0.8086 - learning_rate: 5.0000e-05\n",
      "Epoch 2/5\n",
      "\u001b[1m623/623\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9985 - auc: 1.0000 - loss: 0.4647 - precision: 0.9989 - recall: 0.9982\n",
      "Epoch 2: val_accuracy did not improve from 0.82509\n",
      "\u001b[1m623/623\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m999s\u001b[0m 2s/step - accuracy: 0.9980 - auc: 1.0000 - loss: 0.4595 - precision: 0.9984 - recall: 0.9976 - val_accuracy: 0.8130 - val_auc: 0.9401 - val_loss: 0.8432 - val_precision: 0.8260 - val_recall: 0.8038 - learning_rate: 5.0000e-05\n",
      "Epoch 3/5\n",
      "\u001b[1m623/623\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9973 - auc: 1.0000 - loss: 0.4458 - precision: 0.9977 - recall: 0.9954\n",
      "Epoch 3: val_accuracy did not improve from 0.82509\n",
      "\u001b[1m623/623\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1036s\u001b[0m 2s/step - accuracy: 0.9968 - auc: 0.9999 - loss: 0.4424 - precision: 0.9973 - recall: 0.9951 - val_accuracy: 0.8134 - val_auc: 0.9301 - val_loss: 0.8730 - val_precision: 0.8202 - val_recall: 0.8070 - learning_rate: 5.0000e-05\n",
      "Epoch 4/5\n",
      "\u001b[1m623/623\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9992 - auc: 1.0000 - loss: 0.4254 - precision: 0.9996 - recall: 0.9987\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.82509\n",
      "\u001b[1m623/623\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1057s\u001b[0m 2s/step - accuracy: 0.9983 - auc: 1.0000 - loss: 0.4229 - precision: 0.9991 - recall: 0.9979 - val_accuracy: 0.7728 - val_auc: 0.9220 - val_loss: 0.9578 - val_precision: 0.7764 - val_recall: 0.7551 - learning_rate: 5.0000e-05\n",
      "Epoch 5/5\n",
      "\u001b[1m623/623\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9981 - auc: 1.0000 - loss: 0.4136 - precision: 0.9988 - recall: 0.9980\n",
      "Epoch 5: val_accuracy did not improve from 0.82509\n",
      "\u001b[1m623/623\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1052s\u001b[0m 2s/step - accuracy: 0.9989 - auc: 1.0000 - loss: 0.4118 - precision: 0.9991 - recall: 0.9988 - val_accuracy: 0.8122 - val_auc: 0.9436 - val_loss: 0.8185 - val_precision: 0.8215 - val_recall: 0.7973 - learning_rate: 1.0000e-05\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "ğŸš€ Phase 3: Unfreezing last 40 layers with lr=1e-05...\n",
      "âœ… Top 40 layers set to trainable for fine-tuning.\n",
      "âœ… Model recompiled after unfreezing layers (lr=1e-05).\n",
      "Epoch 1/5\n",
      "\u001b[1m623/623\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9980 - auc: 1.0000 - loss: 0.4690 - precision: 0.9989 - recall: 0.9962\n",
      "Epoch 1: val_accuracy improved from 0.82509 to 0.84278, saving model to artifacts\\training\\model.keras\n",
      "\u001b[1m623/623\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1077s\u001b[0m 2s/step - accuracy: 0.9983 - auc: 1.0000 - loss: 0.4660 - precision: 0.9988 - recall: 0.9971 - val_accuracy: 0.8428 - val_auc: 0.9572 - val_loss: 0.7927 - val_precision: 0.8511 - val_recall: 0.8319 - learning_rate: 1.0000e-05\n",
      "Epoch 2/5\n",
      "\u001b[1m623/623\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9992 - auc: 1.0000 - loss: 0.4590 - precision: 0.9997 - recall: 0.9987\n",
      "Epoch 2: val_accuracy did not improve from 0.84278\n",
      "\u001b[1m623/623\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1106s\u001b[0m 2s/step - accuracy: 0.9988 - auc: 0.9999 - loss: 0.4580 - precision: 0.9995 - recall: 0.9982 - val_accuracy: 0.8388 - val_auc: 0.9517 - val_loss: 0.8057 - val_precision: 0.8476 - val_recall: 0.8231 - learning_rate: 1.0000e-05\n",
      "Epoch 3/5\n",
      "\u001b[1m623/623\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9990 - auc: 1.0000 - loss: 0.4522 - precision: 0.9991 - recall: 0.9985\n",
      "Epoch 3: val_accuracy did not improve from 0.84278\n",
      "\u001b[1m623/623\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m988s\u001b[0m 2s/step - accuracy: 0.9991 - auc: 1.0000 - loss: 0.4506 - precision: 0.9994 - recall: 0.9988 - val_accuracy: 0.8279 - val_auc: 0.9490 - val_loss: 0.8088 - val_precision: 0.8403 - val_recall: 0.8102 - learning_rate: 1.0000e-05\n",
      "Epoch 4/5\n",
      "\u001b[1m623/623\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9986 - auc: 1.0000 - loss: 0.4474 - precision: 0.9992 - recall: 0.9976\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 1.9999999494757505e-06.\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.84278\n",
      "\u001b[1m623/623\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m987s\u001b[0m 2s/step - accuracy: 0.9989 - auc: 1.0000 - loss: 0.4453 - precision: 0.9994 - recall: 0.9980 - val_accuracy: 0.8396 - val_auc: 0.9558 - val_loss: 0.7890 - val_precision: 0.8444 - val_recall: 0.8247 - learning_rate: 1.0000e-05\n",
      "Epoch 5/5\n",
      "\u001b[1m623/623\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9998 - auc: 1.0000 - loss: 0.4399 - precision: 1.0000 - recall: 0.9996\n",
      "Epoch 5: val_accuracy did not improve from 0.84278\n",
      "\u001b[1m623/623\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1127s\u001b[0m 2s/step - accuracy: 0.9997 - auc: 1.0000 - loss: 0.4403 - precision: 0.9999 - recall: 0.9993 - val_accuracy: 0.8388 - val_auc: 0.9531 - val_loss: 0.7922 - val_precision: 0.8456 - val_recall: 0.8283 - learning_rate: 2.0000e-06\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "âœ… Model saved at: artifacts\\training\\model.keras\n",
      "âœ… Model saved at: artifacts\\training\\model.keras\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    # 1ï¸âƒ£ Load configuration\n",
    "    config = ConfigurationManager()\n",
    "    training_config = config.get_training_config()\n",
    "\n",
    "    # 2ï¸âƒ£ Initialize Training\n",
    "    training = Training(config=training_config)\n",
    "\n",
    "    # 3ï¸âƒ£ Load the updated base model (with custom head)\n",
    "    training.get_base_model()\n",
    "\n",
    "    # 4ï¸âƒ£ (Optional) Unfreeze last N layers for fine-tuning\n",
    "    training.unfreeze_top_layers(num_layers=training_config.params_fine_tune_last_n)\n",
    "\n",
    "    # 5ï¸âƒ£ Prepare training and validation generators\n",
    "    training.train_valid_generator()\n",
    "\n",
    "    # 6ï¸âƒ£ Train the model with callbacks (EarlyStopping, ReduceLROnPlateau, ModelCheckpoint)\n",
    "    history = training.train()\n",
    "\n",
    "    # 7ï¸âƒ£ Save the final fine-tuned model\n",
    "    training.save_model(training_config.trained_model_path, training.model)\n",
    "\n",
    "except Exception as e:\n",
    "    raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba5f419",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kidney",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
